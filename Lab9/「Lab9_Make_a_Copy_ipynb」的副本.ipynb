{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdDimpLi2_nI"
      },
      "source": [
        "# **Lecture 9 - Step-by-Step implementation and usage of LSTM**\n",
        "\n",
        "In this short tutorial, I will show you how to use PyTorch's implementation of LSTM. I will use the same data used in Lab 8.\n",
        "\n",
        "This tutorial is adapted from [Chapter 9](https://d2l.ai/chapter_recurrent-neural-networks/index.html),  [Chapter 10](https://classic.d2l.ai/chapter_convolutional-neural-networks/index.html) of the textbook, and from [this PyTorch tutorial on classifying names](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html).\n",
        "\n",
        "Section 1 is the same as Section 1.1. of Topic 8's tutorial. Section 2 is new, while the remaining sections are the same Topuic's 8 tutorial. In this notebook, I included all the sections for completeness."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Using LSTM**\n",
        "\n"
      ],
      "metadata": {
        "id": "OH2vyEYp2R0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3CHhtRcXtFL",
        "outputId": "7c1db472-8871-4b8e-fb69-a6b84658972e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ff7dc9954d0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Hello LSTM\n",
        "\n",
        "# Create an LSTM whose input is of dim 3 and has an output (i.e., no. of features/dim in the hidden state) of dim 3\n",
        "lstm = nn.LSTM(3, 3)\n",
        "\n",
        "# Let's make a random sequence of\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
        "for i in range(5):\n",
        "  print(inputs[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYbus-MgXv4r",
        "outputId": "9993c7a1-2b96-45b8-e4b8-1ff4a341bfa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1473,  0.3482,  1.1371]])\n",
            "tensor([[-0.3339, -1.4724,  0.7296]])\n",
            "tensor([[-0.1312, -0.6368,  1.0429]])\n",
            "tensor([[ 0.4903,  1.0318, -0.5989]])\n",
            "tensor([[ 1.6015, -1.0735, -1.2173]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just for test, let's feed this input into the LSTM and output the predictions (note that the network has not been trained yet). There are two ways of doing this;\n",
        "\n"
      ],
      "metadata": {
        "id": "AE2gjQZiZjaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the hidden state and cell state (memory of the LSTM unit).\n",
        "hidden = (torch.randn(1, 1, 3),\n",
        "          torch.randn(1, 1, 3))\n",
        "\n",
        "# Stepping through the sequence one element at a time\n",
        "for i in inputs:\n",
        "    print(i)\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "\n",
        "# Displaying the prediction\n",
        "print(out)      # the final output\n",
        "print(hidden)   # the hidden state of  the network\n"
      ],
      "metadata": {
        "id": "R9CPm8UnZ4Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Approach 2: Process the entire sequence at once**\n",
        "\n",
        "In this case, LSTM returns\n",
        "- out: All of the hidden states throughout the sequence,\n",
        "- hidden: The most recent hidden state.\n",
        "\n",
        "In this case, the input to LSTM is one tensor whose dimensions are as follows:\n",
        "- Dim 1 (first dimensions): corresponds to the length of the sequence\n",
        "- Dim 2: mini batch size - let's consider it as 1 for now.\n",
        "- Dim 3: the length of each element in the sequence."
      ],
      "metadata": {
        "id": "k3bBHH7OaaWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all the input to form one tensor of size sequence_length x minibatch_size x element_length\n",
        "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "\n",
        "# Initialize the hidden state\n",
        "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))\n",
        "\n",
        "# Run LSTm through  the entire sequence (inputs) at once\n",
        "out, hidden = lstm(inputs, hidden)\n",
        "print(out)\n",
        "print(hidden)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbqpwJm-ZNFi",
        "outputId": "de217591-cde8-49a5-e66c-249fa7fb585b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1149,  0.0220,  0.2672]],\n",
            "\n",
            "        [[ 0.0150,  0.0030,  0.0964]],\n",
            "\n",
            "        [[-0.1190,  0.0397,  0.0507]],\n",
            "\n",
            "        [[-0.0874,  0.0521,  0.0937]],\n",
            "\n",
            "        [[ 0.1593, -0.0071, -0.0020]]], grad_fn=<StackBackward0>)\n",
            "(tensor([[[ 0.1593, -0.0071, -0.0020]]], grad_fn=<StackBackward0>), tensor([[[ 0.2186, -0.0347, -0.0096]]], grad_fn=<StackBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can cascade multiple LSTM blocks together. The number of blocks (called layers in LSTM) can be specified inthe third argument of nn.LSTM.\n",
        "\n",
        "In the example below, we cascade two LSTM blocks:"
      ],
      "metadata": {
        "id": "50_I5lEZdTRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an LSTM whose input is of dim 3 and has an output (i.e., no. of features/dim in the hidden state) of dim 3\n",
        "n_input = 3\n",
        "n_hidden = 3\n",
        "n_blocks = 2\n",
        "lstm = nn.LSTM(n_input, n_hidden, n_blocks)\n",
        "\n",
        "# Let's make a random sequence of\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
        "#for i in range(5):\n",
        "#  print(inputs[i])\n",
        "\n",
        "# Concatenate all the input to form one tensor of size sequence_length x minibatch_size x element_length\n",
        "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "\n",
        "# Run LSTm through  the entire sequence (inputs) at once\n",
        "out, hidden = lstm(inputs)  #, hidden)\n",
        "print(out)\n",
        "print(hidden)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvC93IMYd1ef",
        "outputId": "7ec2971a-b339-4084-832f-ba1729e0d299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0196,  0.0525,  0.2091]],\n",
            "\n",
            "        [[-0.0228,  0.0760,  0.3162]],\n",
            "\n",
            "        [[-0.0192,  0.1160,  0.3595]],\n",
            "\n",
            "        [[ 0.0344,  0.1436,  0.3846]],\n",
            "\n",
            "        [[-0.0107,  0.1668,  0.3867]]], grad_fn=<StackBackward0>)\n",
            "(tensor([[[-0.2989, -0.0129,  0.0624]],\n",
            "\n",
            "        [[-0.0107,  0.1668,  0.3867]]], grad_fn=<StackBackward0>), tensor([[[-0.5663, -0.0504,  0.1610]],\n",
            "\n",
            "        [[-0.0172,  0.4029,  1.4973]]], grad_fn=<StackBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Your Task**\n",
        "\n",
        "Integrate this code into the classes you created last week and use it to recognize the language of surnames (please refer to last week's lab)."
      ],
      "metadata": {
        "id": "RjMp3h7oiBsN"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}