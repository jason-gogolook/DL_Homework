{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jason-gogolook/DL_Homework/blob/main/Lab8_RNNs_Make_a_Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdDimpLi2_nI"
      },
      "source": [
        "# **Lab 8 - Recurrent Neural Networks (RNNs)**\n",
        "\n",
        "This tutorial demonstrates how to implement and use Recurrent Neural Networks (RNNs). We will first start with a the implementatiom from scratch of a simple RNN. Then, we will use PyTorch libraries to rtain a Long Short-Term Memory (LSTM) network. LSTMs are widely used for processing sequential data such as text, videos, etc..\n",
        "\n",
        "This tutorial is adapted from [Chapter 9](https://d2l.ai/chapter_recurrent-neural-networks/index.html) and  [Chapter 10](https://classic.d2l.ai/chapter_convolutional-neural-networks/index.html) of the textbook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Simple RNN from scratch**\n",
        "\n",
        "This tutorial is adapted from [this PyTorch tutorial on classifying names](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html). Specifically, we build and train a simple RNN that takes surnames and predicts which language the name is from based on its spelling. We consider $18$ languages."
      ],
      "metadata": {
        "id": "OH2vyEYp2R0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1. Data preparation**\n",
        "\n",
        "First, download the data from [here](https://download.pytorch.org/tutorial/data.zip) and extract it into the current directory. In my case, I named the folder `language_data`. You will notice that the directory `language_data/name` contains $18$ text files (one for each language) named as `[Language].txt`. Each file contains a bunch of names, one name per line, mostly romanized (but we still need to convert from Unicode to ASCII).\n",
        "\n",
        "First, we need to:\n",
        "- Convert the names from Unicode into ASCII.\n",
        "- Build up a dictionary of names per language, in the form `{language: [name1, name2, ...]}`.\n",
        "\n"
      ],
      "metadata": {
        "id": "n_X4qHj-kjEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob   # short for global. It is used to return all file paths that match a specific pattern.\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "file_path = 'drive/My Drive/DL Course/data/names/*.txt'\n",
        "\n",
        "# Find all file names within a given path\n",
        "def findFiles(path):\n",
        "  return glob.glob(path)\n",
        "\n",
        "# You just need to run the comman below once\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Testing\n",
        "print(findFiles(file_path))\n"
      ],
      "metadata": {
        "id": "qEd-5USmoF_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8a364b-9242-440f-beba-d36e1a226f15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "['drive/My Drive/DL Course/data/names/English.txt', 'drive/My Drive/DL Course/data/names/Dutch.txt', 'drive/My Drive/DL Course/data/names/French.txt', 'drive/My Drive/DL Course/data/names/Irish.txt', 'drive/My Drive/DL Course/data/names/Japanese.txt', 'drive/My Drive/DL Course/data/names/Italian.txt', 'drive/My Drive/DL Course/data/names/Polish.txt', 'drive/My Drive/DL Course/data/names/Spanish.txt', 'drive/My Drive/DL Course/data/names/Portuguese.txt', 'drive/My Drive/DL Course/data/names/Scottish.txt', 'drive/My Drive/DL Course/data/names/Chinese.txt', 'drive/My Drive/DL Course/data/names/Czech.txt', 'drive/My Drive/DL Course/data/names/Korean.txt', 'drive/My Drive/DL Course/data/names/German.txt', 'drive/My Drive/DL Course/data/names/Russian.txt', 'drive/My Drive/DL Course/data/names/Greek.txt', 'drive/My Drive/DL Course/data/names/Arabic.txt', 'drive/My Drive/DL Course/data/names/Vietnamese.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "device = get_default_device()\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wFF6ADPHLwe",
        "outputId": "64693f1e-b674-4a95-bb8d-e07133bb77bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we write a function that turns Unicode strings into plain ASCII."
      ],
      "metadata": {
        "id": "4MJtQuHor-k6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import string\n",
        "\n",
        "# Turning Unicode string to pain ASCII\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "print(unicodeToAscii('Ślusàrski'))"
      ],
      "metadata": {
        "id": "UmQuuWoerteL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd151644-5a99-42d9-fe2b-1c0ef9341ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slusarski\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we would like to build the `category_lines` dictionary, which is a list of names per language."
      ],
      "metadata": {
        "id": "Q0yj8pxysXbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "# Read a file and split into lines\n",
        "def readLines(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "for filename in findFiles(file_path):\n",
        "    # extract filename as the category name\n",
        "    category = os.path.splitext(os.path.basename(filename))[0]\n",
        "    all_categories.append(category)\n",
        "\n",
        "    # read lines from the category, which are surnames of that category\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)\n",
        "\n",
        "## Just for test\n",
        "# print all the Chinese names\n",
        "print(category_lines['Chinese'])\n",
        "\n",
        "# Print only the first 5 Chinese names\n",
        "print(category_lines['Chinese'][:5])"
      ],
      "metadata": {
        "id": "qvhdOcAvsp-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next**, we need to turn the names into Tensors so that we can use them. We use \"on-hot-vector\" representation. For example, when we have an alphabet of $26$ characters (which is the case in English), we represent each letter of the alphabet as a vector of length  $26$. It will be filled with zeros except for the one at the index of the current letter. For example, the letter \"b\" will be represented as `<0 1 0 0 ...>`.\n",
        "\n",
        "To make a word, e.g., `Hamid`, we join the one-hot-vectors of each of the letters into a 2D matrix of size `word_length X 1 X n_alphabet_letters`. The extra 1 dimension is because PyTorch assumes everything is in batches - we’re just using a batch size of 1 here."
      ],
      "metadata": {
        "id": "-onvNOHZt8MP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def letterToTensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    tensor[0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "## Testing\n",
        "# print(letterToTensor('J'))\n",
        "\n",
        "# print(lineToTensor('Jones').size())\n",
        "\n",
        "print(lineToTensor('Jones'))"
      ],
      "metadata": {
        "id": "KmQCkBSGvjFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2. Creating the network**\n",
        "We will use a simple RNN module which takes the input and the hidden state,  concatenates them, and feeds them into:\n",
        "- The input-to-output branch (i2o) composed of a linear layer followed by a LogSoftMax layer (activation function) to produce the output\n",
        "- The input-to-hidden branch (i2h) composed of a linear layer. It outputs the hidden state, which is fed back to the network (the recurrent loop).\n",
        "\n",
        "The code below shows how to create such RNN module."
      ],
      "metadata": {
        "id": "1oOj7XN6wFAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, lr):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lr = lr\n",
        "\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "      # Note here that the forward step takes as input the input and the hidden state\n",
        "      # It then combines them bby concatenation before feeding them to the network\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        # the hidden state needs to be initialized (for the firs iteration)\n",
        "        return torch.zeros(1, self.hidden_size).to(device)\n",
        "\n",
        "    ## The loss function - Here, we will use Negative Log Likelihood\n",
        "    def loss(self, y_hat, y):\n",
        "      fn = nn.NLLLoss()\n",
        "      return fn(y_hat, y)\n",
        "\n",
        "    ## The optimization algorithm\n",
        "    def configure_optimizers(self):\n",
        "      return torch.optim.Adam(self.parameters(), self.lr)\n",
        "\n",
        "    def evaluate(self, X):\n",
        "      hidden  = self.initHidden()\n",
        "\n",
        "      for i in range(X.size()[0]):\n",
        "        output, hidden = self.forward(X[i], hidden)\n",
        "\n",
        "      return output\n",
        "\n"
      ],
      "metadata": {
        "id": "--BBCAqj1Eoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do a quick test;"
      ],
      "metadata": {
        "id": "xdMuQo5r1jSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(n_letters)\n",
        "print(n_categories)"
      ],
      "metadata": {
        "id": "EzNX-FR71mFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run a step of this network we need to pass an input (in our case, the Tensor for the current letter) and a previous hidden state (which we initialize as zeros at first)."
      ],
      "metadata": {
        "id": "LDGoy4Qg3koX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# size of the hidden state\n",
        "n_hidden = 128\n",
        "\n",
        "input = lineToTensor('Albert')\n",
        "input = input.to(device)\n",
        "\n",
        "# As it is the first step, we need to initialize the hidden layer\n",
        "\n",
        "# For RNN\n",
        "hidden = torch.zeros(1, n_hidden).to(device)\n",
        "model = RNN(n_letters, n_hidden, n_categories, lr = 1e-04)\n",
        "model = model.to(device)\n",
        "\n",
        "output, next_hidden = model(input[0], hidden)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "PKJ08rvV3rRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By running the code above, you will notice that the output is a Tensor of size `1 X n_categories`, where every item is the likelihood of that category (higher is more likely). This, however, is not interpretable. In fact, we need to get from that output, the index of the element that has the maximum likelihood and use that index to find the name of the corresponding language.  This can be done using the following helper function:"
      ],
      "metadata": {
        "id": "lICw65dy5OdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categoryFromOutput(output):\n",
        "    top_n, top_i = output.topk(1)\n",
        "    category_i = top_i[0].item()\n",
        "    return all_categories[category_i], category_i\n",
        "\n",
        "print(categoryFromOutput(output))\n"
      ],
      "metadata": {
        "id": "A54cMNSi5vTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3. Training**\n",
        "Each loop of training,\n",
        "- Creates input and target tensors\n",
        "- Creates a zeroed initial hidden state\n",
        "- Reads each letter in and\n",
        "- Keeps hidden state for next letter\n",
        "- Compares final output to target\n",
        "- Back-propagates the gradient\n",
        "- Returns the output and loss"
      ],
      "metadata": {
        "id": "OEnXIm1N42Dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first make a mechanism for picking random training samples from our  training dataset."
      ],
      "metadata": {
        "id": "JRVqaLXoQVDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A mechanism for picking a random training sample\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set Random Seed\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "def randomTrainingExample(all_categories, category_lines):\n",
        "    # pick up a random language\n",
        "    category = randomChoice(all_categories)\n",
        "\n",
        "    # pick up a random name from that language\n",
        "    line = randomChoice(category_lines[category])\n",
        "\n",
        "    # Convert the picked data into a tensor\n",
        "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
        "    line_tensor = lineToTensor(line)\n",
        "\n",
        "    return category, line, category_tensor.to(device), line_tensor.to(device)\n",
        "\n",
        "## Let's test it\n",
        "for i in range(1):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample(all_categories, category_lines)\n",
        "    print('category =', category, '/ line =', line)\n",
        "    print(category_tensor)\n",
        "    print(line_tensor)"
      ],
      "metadata": {
        "id": "6X-3hQ07CwKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create the training class."
      ],
      "metadata": {
        "id": "vO03lAxNRAq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "## The training loop\n",
        "class Trainer:\n",
        "\n",
        "  def __init__(self, n_epochs = 1):\n",
        "    self.max_epochs = n_epochs\n",
        "    # self.writer     = tb  # the tensorboard instance\n",
        "\n",
        "  def fit(self, model, all_categories, category_lines):\n",
        "\n",
        "    self.current_loss = 0\n",
        "    self.all_losses   = []\n",
        "\n",
        "    self.all_categories = all_categories\n",
        "    self.category_lines = category_lines\n",
        "\n",
        "    # Trasnfer the model to the device (GPU or CPU)\n",
        "    model.to(device)\n",
        "\n",
        "    # configure the optimizer\n",
        "    self.optimizer = model.configure_optimizers()\n",
        "    self.model     = model\n",
        "\n",
        "    self.start = time.time()\n",
        "\n",
        "    for epoch in range(self.max_epochs):\n",
        "      self.fit_epoch()\n",
        "\n",
        "      # Logging the average training loss so that it can be visualized in the tensorboard\n",
        "      # self.writer.add_scalar(\"Training Loss\", self.avg_training_loss, epoch)\n",
        "\n",
        "    print(\"Training process has finished\")\n",
        "\n",
        "  def fit_epoch(self):\n",
        "\n",
        "    n_iters = 100000;\n",
        "    print_every = 5000\n",
        "    plot_every  = 1000\n",
        "\n",
        "    self.current_loss = 0.0\n",
        "    self.all_losses = []\n",
        "\n",
        "    # self.avg_training_loss = 0.0\n",
        "\n",
        "    # iterate over the DataLoader for training data\n",
        "    for iter in range(1, n_iters+1):\n",
        "\n",
        "      ## Get input\n",
        "      category, line, category_tensor, line_tensor = randomTrainingExample(all_categories, category_lines)\n",
        "\n",
        "      ## training\n",
        "      hidden = self.model.initHidden()\n",
        "\n",
        "      # Clear gradient buffers because we don't want any gradient from previous\n",
        "      # epoch to carry forward, dont want to cummulate gradients\n",
        "      self.optimizer.zero_grad()\n",
        "\n",
        "      # get output from the model, given the inputs\n",
        "      for i in range(line_tensor.size()[0]):\n",
        "          output, hidden = self.model(line_tensor[i], hidden)\n",
        "\n",
        "      # get loss for the predicted output\n",
        "      loss = self.model.loss(output, category_tensor)\n",
        "\n",
        "      # get gradients w.r.t to the parameters of the model\n",
        "      loss.backward()\n",
        "\n",
        "      # update the parameters (perform optimization)\n",
        "      self.optimizer.step()\n",
        "\n",
        "      ## Let's print some statistics - Gradient is not required from here\n",
        "      with torch.no_grad():\n",
        "        self.current_loss += loss\n",
        "\n",
        "        # Print the iteration no., loss, name and guess\n",
        "        if iter % print_every == 0:\n",
        "          guess, guess_i = categoryFromOutput(output)\n",
        "\n",
        "          correct = '✓' if guess == category else '✗ (%s)' % category\n",
        "          print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(self.start), loss, line, guess, correct))\n",
        "\n",
        "        # Add current loss avg to list of losses (avergae loss of \"plot_every\" iterations)\n",
        "        if iter % plot_every == 0:\n",
        "            self.all_losses.append(self.current_loss / plot_every)\n",
        "            self.current_loss = 0"
      ],
      "metadata": {
        "id": "0vv-XmTpho_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have all what we need to train the network."
      ],
      "metadata": {
        "id": "7n3BUy7uQaXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. The RNN model\n",
        "n_hidden = 128\n",
        "model = RNN(n_letters, n_hidden, n_categories, lr=1e-04)\n",
        "\n",
        "# 3. Training the network\n",
        "# 3.1. Creating the trainer class - note that here, I passed writer as a  parameter to the trainer\n",
        "trainer = Trainer(n_epochs=1)\n",
        "\n",
        "# 3.2. Training the model\n",
        "trainer.fit(model, all_categories, category_lines)"
      ],
      "metadata": {
        "id": "YlgMICn-QdKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting the results**\n",
        "\n",
        "Plotting the historical loss from all_losses shows the network learning."
      ],
      "metadata": {
        "id": "TT8RM2Yvg1EO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "all_losses = trainer.all_losses\n",
        "\n",
        "# Convert tensors back to cpu because numpy is unable to use gpus\n",
        "all_losses_cpu = [loss.cpu().item() for loss in all_losses]\n",
        "plt.plot(all_losses_cpu)"
      ],
      "metadata": {
        "id": "iv79_D-2gshe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note also that you can use TensorBoard to do this plotting."
      ],
      "metadata": {
        "id": "qdRY4jzoVhIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the results**\n",
        "\n",
        "To see how well the network performs on different categories, we will create a confusion matrix, indicating for every actual language (rows) which language the network guesses (columns). To calculate the confusion matrix a bunch of samples are run through the network with evaluate(), which is the same as fit() minus the backprop."
      ],
      "metadata": {
        "id": "6xVbrinfZQCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep track of correct guesses in a confusion matrix\n",
        "confusion = torch.zeros(n_categories, n_categories)\n",
        "n_confusion = 10000\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Go through a bunch of examples and record which are correctly guessed\n",
        "for i in range(n_confusion):\n",
        "  category, line, category_tensor, line_tensor = randomTrainingExample(all_categories, category_lines)\n",
        "  output = model.evaluate(line_tensor)\n",
        "\n",
        "  guess, guess_i = categoryFromOutput(output)\n",
        "  category_i = all_categories.index(category)\n",
        "  confusion[category_i][guess_i] += 1\n",
        "\n",
        "# Normalize by dividing every row by its sum\n",
        "for i in range(n_categories):\n",
        "    confusion[i] = confusion[i] / confusion[i].sum()\n",
        "\n",
        "# Set up plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(confusion.numpy())\n",
        "fig.colorbar(cax)\n",
        "\n",
        "# Set up axes\n",
        "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
        "ax.set_yticklabels([''] + all_categories)\n",
        "\n",
        "# Force label at every tick\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "# sphinx_gallery_thumbnail_number = 2\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rMzEeSr8XSqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check whether the network performs well, the confusion matrix should have high values along the diagonal elements (ideally 1) and low values (ideally 0) in off-diagonal elements.\n",
        "\n",
        "What do you think about the performance of your network? What can you do to improve it?"
      ],
      "metadata": {
        "id": "k_BDsn5bZXUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. LSTM**\n",
        "\n",
        "Now, update the code above so that instead of using a simple RNN, we will use LSTM. Start with one block LSTM and then try to cascade multiple LSTM blocks and compare the performance (e.g., by looking at the confusion matrix).\n",
        "\n",
        "For using LSTM, please refer to this [LSTM tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html). Make sure you implement it in a modular way following the structure we defined so far."
      ],
      "metadata": {
        "id": "YxGnQ7eZaDzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example"
      ],
      "metadata": {
        "id": "MA1CDS-iLk3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch’s LSTM expects all of its inputs to be 3D tensors. The semantics of the axes of these tensors is important. The first axis is the sequence itself, the second indexes instances in the mini-batch, and the third indexes elements of the input."
      ],
      "metadata": {
        "id": "DCp88dC2Le9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idim = 3 # Input dim is 3\n",
        "odim = 5 # Output dim is 4\n",
        "\n",
        "lstm = nn.LSTM(idim, odim)  # layers is 1 (default), batch_first=False\n",
        "\n",
        "# Sequence length\n",
        "seq_size = 1\n",
        "\n",
        "# make a sequence of seq_size number of (1,idim) row vectors\n",
        "inputs = [torch.randn(1, idim) for _ in range(seq_size)]\n",
        "print(inputs)\n",
        "\n",
        "# hidden is a tuple containing the initial hidden state H\n",
        "# and the initial cell state C of the LSTM.\n",
        "# The hidden tuple has two elements. Each element has a shape of (1, 1, odim).\n",
        "# The dimensions represent (num_layers, batch_size, hidden_size).\n",
        "# Here, num_layers is 1 (the default value), batch_size is 1\n",
        "# (since batch_first=False by default), and hidden_size is odim, which is\n",
        "# the output dimension of the LSTM.\n",
        "hidden = (torch.randn(1, 1, odim),\n",
        "          torch.randn(1, 1, odim))\n",
        "\n",
        "for i in inputs:\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    out, hidden = lstm(i.view(1, 1, idim), hidden)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrI2AWF9LgHR",
        "outputId": "5910f767-816f-4be7-cb31-661e7eb9ed6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 1.0116, -0.1091, -0.9262]])]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}